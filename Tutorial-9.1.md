# Interactive Tutorial 9.1 Autoscaling Deployments with GKE workload metrics 

## Overview
This tutorial demonstrates how to automatically scale your Google Kubernetes Engine (GKE) workloads based on custom metrics emitted by your application (for instance, the number of accounts actively logged in or the number of HTTP requests).
You use the GKE workload metrics pipeline to collect metrics emitted by your application, send them to Cloud Monitoring, and then use them to power the Horizontal Pod Autoscaler (HPA).

**Objectives**
This tutorial covers the following tasks:
* How to deploy an example application which emits Prometheus-style metrics.
* How to deploy a PodMonitor resource to scrape the metrics from your application and publish them to Cloud Monitoring.
* How to deploy the Custom Metrics Adapter.
* How to query workload metrics using the Kubernetes Custom Metrics API.
* How to deploy a Horizontal Pod Autoscaler (HPA) resource to scale your application based on the workload metrics scraped from your application.

## Before you begin
Take the following steps to enable the Kubernetes Engine API:
* Visit the Kubernetes Engine page in the Google Cloud Console.
* Create or select a project.
* Wait for the API and related services to be enabled. This can take several minutes.
* Make sure that billing is enabled for your Cloud project. 

## Step 1. Setting up your environment
To create a new cluster with workload metrics enabled, use the following command but Replace the following with your own details:
CLUSTER_NAME: the name of your cluster.
PROJECT_ID: the ID of your Google Cloud project.
ZONE: Choose a zone that's closest to you.
```
gcloud beta container clusters create CLUSTER_NAME \
 --project=PROJECT_ID \
 --zone=ZONE \
 --monitoring=SYSTEM,WORKLOAD
```
 
Deploying an example application which emits Prometheus-style metrics
Download the repository containing the application code for this tutorial:
```
  git clone https://github.com/papa107/kubernetes-engine-samples.git
  cd kubernetes-engine-samples/workload-metrics
```
The example application for this tutorial generates two metrics and exposes them via a built-in Prometheus endpoint at localhost:1234/metrics:

* example_requests_total: a counter of requests generated by the application polling itself

* example_random_numbers: a histogram of randomly generated numbers

Note: The repository contains a Kubernetes manifest to deploy the application to your cluster:
workload-metrics/manifests/workload-metrics-deployment.yaml

## Step 2. Deploy the application on your cluster:
To deploy the application to your cluster type in the following commands:
```
kubectl create namespace gke-workload-metrics
kubectl apply -f manifests/workload-metrics-deployment.yaml
```
After waiting a moment for the application to deploy, to allow Pods to reach the Ready state, then enter the following command:
```
  kubectl -n gke-workload-metrics get pods
```

Output:

 |                NAME               | READY |  STATUS | RESTARTS | AGE |
|:---------------------------------:|:-----:|:-------:|----------|-----|
| workload-metrics-74fb6c56df-9djq7 |  1/1  | Running | 0        | 1m  |

## Step 3. Deploying a PodMonitor resource to scrape the metrics from the example application
To collect the metrics emitted from the example application, you need to create a PodMonitor custom resource.

The repository contains a Kubernetes manifest to deploy the PodMonitor to your cluster:
workload-metrics/manifests/workload-metrics-podmon.yaml

Deploy the PodMonitor on your cluster:
```
kubectl apply -f manifests/workload-metrics-podmon.yaml
```
 
## Step 4. Deploying the Custom Metrics Adapter
The Custom Metrics Adapter lets your cluster send and receive metrics with Monitoring.
4.1 Grant your user the ability to create required authorization roles:
```
kubectl create clusterrolebinding cluster-admin-binding \
 --clusterrole cluster-admin --user "$(gcloud config get-value account)"
```

 4.2 Deploy the new resource model adapter on your cluster:
```
kubectl apply -f manifests/adapter_new_resource_model.yaml
```
4.3 Check that the custom metrics adapter is deployed and in the Ready state:
```
kubectl -n custom-metrics get pods
```

Output:

|                         NAME                        | READY |  STATUS | RESTARTS | AGE |
|:---------------------------------------------------:|:-----:|:-------:|----------|-----|
| custom-metrics-stackdriver-adapter-6d4fc94699-zqndq |  1/1  | Running | 0        | 2m  |

## Step 5. Querying workload metrics using the Kubernetes Custom Metrics API
You can use the Kubernetes Custom Metrics API to check that your workload metrics are visible to GKE.
GKE workload metrics are exported to Monitoring with the prefix workload.googleapis.com. The Kubernetes Custom Metrics API server does not support the / character in metrics paths, so you need to replace all / characters with |. 
You therefore must use workload.googleapis.com|example_request_total for the metric name.
After waiting a few moments for the applications metrics to be sent to Monitoring, run the following command to query for the workload.googleapis.com|example_request_total metric:
```
kubectl get --raw  \   "/apis/custom.metrics.k8s.io/v1beta2/namespaces/gke-workload-metrics/pods/*/workload.googleapis.com|example_requests_total"
```

Output:

```
 {"kind":"MetricValueList","apiVersion":"custom.metrics.k8s.io/v1beta2",
  "metadata":{"selfLink":"/apis/custom.metrics.k8s.io/v1beta2/namespaces/
  gke-workload-metrics/pods/%2A/workload.googleapis.com%7Cexample_requests_total"},
  "items":[{"describedObject":{"kind":"Pod","namespace":"gke-workload-metrics",
  "name":"prom-example-74fb6c56df-9djq7","apiVersion":"/__internal"},"metric":
  {"name":"workload.googleapis.com|example_requests_total","selector":null},"timestamp ":
  "2021-08-23T10:48:45Z","value":"1199m"}]}
```
## Step 6. Deploying a HorizontalPodAutoscaler object
Once you see the workload.googleapis.com|example_requests_total metric in the response payload of the Custom Metrics API in the previous step, you can deploy a Horizontal Pod Autoscaler (HPA) to resize your deployment based on that metric.

Note: The repository contains a Kubernetes manifest to deploy the HPA to your cluster:
workload-metrics/manifests/workload-metrics-hpa.yaml

This HPA sets the minimum pod replicas to 1 and the maximum to 5. It scales your deployment to ensure that the average value of workload.googleapis.com|example_request_total across all pods is 1.
Deploy the HorizontalPodAutoscaler on your cluster: 
```
kubectl apply -f manifests/workload-metrics-hpa.yaml
```

## Step 7 Observing HorizontalPodAutoscaler scaling up
You can periodically check the number of replicas in your deployment and watch it scale to 5 replicas by running the following command:
```
 kubectl -n gke-workload-metrics get pods
```
 
Output:
 |                NAME               | READY |  STATUS | RESTARTS | AGE |
|:---------------------------------:|:-----:|:-------:|----------|-----|
| workload-metrics-74fb6c56df-9djq7 |  1/1  | Running | 0        | 5m  |
| workload-metrics-74fb6c56df-frzbv | 1/1   | Running | 0        | 7m  |
| workload-metrics-74fb6c56df-h26rw | 1/1   | Running | 0        | 8m  |
| workload-metrics-74fb6c56df-kwvx9 | 1/1   | Running | 0        | 10m |
| workload-metrics-74fb6c56df-vvtnn | 1/1   | Running | 0        | 11m |

You can also inspect the state and activity of the Horizontal Pod Autoscaler by running the following command:
```
 kubectl -n gke-workload-metrics describe hpa workload-metrics-hpa
```
**Congratulations!** 

**Clean up**
To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either delete the project that contains the resources, or keep the project and delete the individual resources.
